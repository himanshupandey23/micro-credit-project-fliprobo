{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import missingno\n",
    "import pandas_profiling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are taking a look at the first 5 and last 5 rows of our dataset. It shows that we have a total of 209593 rows and 37 columns present in our dataframe. We have the label column that stores the defaulter and non deafulter values marked with 0 and 1 making this a Classification problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # show all columns in a dataframe\n",
    "pd.set_option('display.max_rows', None) # show all rows in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed the \"Unnamed: 0\" column from the dataset since it was only storing the index data starting from 1 instead of 0 and was not a worthy feature column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns are as follows:\n",
    "\n",
    "label : Flag indicating whether the user paid back the credit amount within 5 days of issuing the loan {1:success, 0:failure}\n",
    "    \n",
    "msisdn : Mobile number of user\n",
    "    \n",
    "aon : Age on cellular network in days\n",
    "    \n",
    "daily_decr30 : Daily amount spent from main account, averaged over last 30 days (in Indonesian Rupiah)\n",
    "    \n",
    "daily_decr90 : Daily amount spent from main account, averaged over last 90 days (in Indonesian Rupiah)\n",
    "    \n",
    "rental30 : Average main account balance over last 30 days\n",
    "    \n",
    "rental90 : Average main account balance over last 90 days\n",
    "    \n",
    "last_rech_date_ma : Number of days till last recharge of main account\n",
    "    \n",
    "last_rech_date_da : Number of days till last recharge of data account\n",
    "    \n",
    "    \n",
    "last_rech_amt_ma : Amount of last recharge of main account (in Indonesian Rupiah)\n",
    "    \n",
    "cnt_ma_rech30 : Number of times main account got recharged in last 30 days\n",
    "    \n",
    "fr_ma_rech30 : Frequency of main account recharged in last 30 days\n",
    "    \n",
    "sumamnt_ma_rech30 : Total amount of recharge in main account over last 30 days (in Indonesian Rupiah)\n",
    "    \n",
    "medianamnt_ma_rech30 : Median of amount of recharges done in main account over last 30 days at user level (in Indonesian Rupiah)\n",
    "    \n",
    "medianmarechprebal30 : Median of main account balance just before recharge in last 30 days at user level (in Indonesian Rupiah)\n",
    "    \n",
    "cnt_ma_rech90 : Number of times main account got recharged in last 90 days\n",
    "    \n",
    "fr_ma_rech90 : Frequency of main account recharged in last 90 days\n",
    "    \n",
    "sumamnt_ma_rech90 : Total amount of recharge in main account over last 90 days (in Indonasian Rupiah)\n",
    "    \n",
    "medianamnt_ma_rech90 : Median of amount of recharges done in main account over last 90 days at user level (in Indonasian Rupiah)\n",
    "    \n",
    "medianmarechprebal90 : Median of main account balance just before recharge in last 90 days at user level (in Indonasian Rupiah)\n",
    "    \n",
    "cnt_da_rech30 : Number of times data account got recharged in last 30 days\n",
    "    \n",
    "fr_da_rech30 : Frequency of data account recharged in last 30 days\n",
    "    \n",
    "cnt_da_rech90 : Number of times data account got recharged in last 90 days\n",
    "    \n",
    "fr_da_rech90 : Frequency of data account recharged in last 90 days\n",
    "    \n",
    "cnt_loans30 : Number of loans taken by user in last 30 days\n",
    "    \n",
    "amnt_loans30 : Total amount of loans taken by user in last 30 days\n",
    "    \n",
    "maxamnt_loans30 : Maximum amount of loan taken by the user in last 30 days\n",
    "    \n",
    "medianamnt_loans30: Median of amounts of loan taken by the user in last 30 days\n",
    "    \n",
    "cnt_loans90 : Number of loans taken by user in last 90 days\n",
    "    \n",
    "amnt_loans90 : Total amount of loans taken by user in last 90 days\n",
    "    \n",
    "maxamnt_loans90 : Maximum amount of loan taken by the user in last 90 days\n",
    "    \n",
    "medianamnt_loans90: Median of amounts of loan taken by the user in last 90 days\n",
    "    \n",
    "payback30 : Average payback time in days over last 30 days\n",
    "    \n",
    "payback90 : Average payback time in days over last 90 days\n",
    "    \n",
    "pcircle : Telecom circle\n",
    "    \n",
    "pdate : Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} Rows and {} Columns in our dataframe\".format(df.shape[0], df.shape[1]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we have no missing values therefore we won't have to worry about handling missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.bar(df, figsize = (25,5), color=\"tab:green\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just ensuring the missing data information with the help of a visual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Rows is {} and number of Columns is {} before dropping duplicates\".format(df.shape[0], df.shape[1]))\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Number of Rows is {} and number of Columns is {} after dropping duplicates\".format(df.shape[0], df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was 1 duplicate record removed from our entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the info method we are able to confirm the non null count details as well as the datatype information. We have 21 float/decimal datatype, 12 integer datatype and 3 object/categorical datatype columns. We will need to convert the object datatype columns to numerical data before we input the information in our machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used the describe method to check the numerical data details. There are 33 columns which have numerical values in them and it looks like the count, mean, standard deviation, minimum value, 25% quartile, 50% quartile, 75% quartile and maximum value are all mostly properly distributed in terms of data points but I do see some abnormality that we will confirm with a visual on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing the statistical description of numeric datatype columns\n",
    "\n",
    "plt.figure(figsize = (20,15))\n",
    "sns.heatmap(round(df.describe()[1:].transpose(),2), linewidth = 2, annot= True, fmt = \".4f\", cmap=\"plasma\")\n",
    "plt.title(\"Satistical Report of Numerical Columns\")\n",
    "plt.xticks(fontsize = 12)\n",
    "plt.yticks(fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above report we can see that the maximum value for columns aon, daily_decr30, daily_decr90, rental30, rental90, last_rech_date_ma, last_rech_date_da, fr_ma_rech30, sumamnt_ma_rech30, medianmarechprebal30, sumamnt_ma_rech90 and fr_da_rech30 have quite a high number than the other column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nunique().sort_values().to_frame(\"Unique Values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above list we can see that column pcircle has 1 single data value filled in all the records and therefore do not conribute much towards the output label generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('pcircle', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have removed the column name \"pcircle\" since it is not contributing towards the label prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the correlation data for our columns in our entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas-profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. It generates interactive reports in web format that can be presented to any person, even if they don’t know programming. It also offers report generation for the dataset with lots of features and customizations for the report generated. In short, what pandas-profiling does is save us all the work of visualizing and understanding the distribution of each variable. It generates a report with all the information easily available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = 'label'\n",
    "    k=0\n",
    "    plt.figure(figsize=[5,7])\n",
    "    axes = sns.countplot(df[x])\n",
    "    for i in axes.patches:\n",
    "        ht = i.get_height()\n",
    "        mr = len(df[x])\n",
    "        st = f\"{ht} ({round(ht*100/mr,2)}%)\"\n",
    "        plt.text(k, ht/2, st, ha='center', fontweight='bold')\n",
    "        k += 1\n",
    "    plt.ylim(0,210000)\n",
    "    plt.title(f'Count Plot for {x} column\\n')\n",
    "    plt.ylabel(f'total number of rows covered\\n')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above count plot we can see that our label data is imbalanced which will need to be balanced before we feed information into our calssification machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = 'maxamnt_loans90'\n",
    "    k=0\n",
    "    plt.figure(figsize=[7,7])\n",
    "    axes = sns.countplot(df[x])\n",
    "    for i in axes.patches:\n",
    "        ht = i.get_height()\n",
    "        mr = len(df[x])\n",
    "        st = f\"{ht} ({round(ht*100/mr,2)}%)\"\n",
    "        plt.text(k, ht/2, st, ha='center', fontweight='bold')\n",
    "        k += 1\n",
    "    plt.ylim(0,210000)\n",
    "    plt.title(f'Count Plot for {x} column\\n')\n",
    "    plt.ylabel(f'total number of rows covered\\n')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    x = 'medianamnt_loans30'\n",
    "    k=0\n",
    "    plt.figure(figsize=[15,7])\n",
    "    axes = sns.countplot(df[x])\n",
    "    for i in axes.patches:\n",
    "        ht = i.get_height()\n",
    "        mr = len(df[x])\n",
    "        st = f\"{ht} ({round(ht*100/mr,2)}%)\"\n",
    "        plt.text(k, ht/2, st, ha='center', fontweight='bold')\n",
    "        k += 1\n",
    "    plt.ylim(0,210000)\n",
    "    plt.title(f'Count Plot for {x} column\\n')\n",
    "    plt.ylabel(f'total number of rows covered\\n')\n",
    "    plt.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above count plots we can see the categories present in the columns along with the percentage and number of rows covered by each unique value of that column in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 'label'\n",
    "\n",
    "x = 'aon'\n",
    "plt.figure(figsize=[15,7])\n",
    "sns.barplot(x,y,data=df,orient='h')\n",
    "plt.title(f\"Barplot for {x} column vs {y} column\")\n",
    "plt.show()\n",
    "\n",
    "x = 'last_rech_date_da'\n",
    "plt.figure(figsize=[15,7])\n",
    "sns.barplot(x,y,data=df,orient='h')\n",
    "plt.title(f\"Barplot for {x} column vs {y} column\")\n",
    "plt.show()\n",
    "\n",
    "x = 'last_rech_date_ma'\n",
    "plt.figure(figsize=[15,7])\n",
    "sns.barplot(x,y,data=df,orient='h')\n",
    "plt.title(f\"Barplot for {x} column vs {y} column\")\n",
    "plt.show()\n",
    "\n",
    "x = 'last_rech_amt_ma'\n",
    "plt.figure(figsize=[15,7])\n",
    "sns.barplot(x,y,data=df,orient='h')\n",
    "plt.title(f\"Barplot for {x} column vs {y} column\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bar plots show the success and failure in returning the credit amount by a user depending on the specified feature columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind=\"line\", x=\"pdate\", y=[\"last_rech_date_da\", \"last_rech_date_ma\", \"last_rech_amt_ma\"], figsize=[15,10])\n",
    "\n",
    "df.plot(kind=\"line\", x=\"msisdn\", y=[\"last_rech_date_da\", \"last_rech_date_ma\", \"last_rech_amt_ma\"], figsize=[15,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have line plots for date and mobile number data with respect to daily and monthly recharge information along with the amount factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='medianamnt_loans30', y='medianamnt_loans90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='maxamnt_loans30', y='maxamnt_loans90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='cnt_da_rech30', y='cnt_da_rech90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='cnt_loans30', y='cnt_loans90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='amnt_loans30', y='amnt_loans90', data=df, hue='label')\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='cnt_ma_rech30', y='cnt_ma_rech90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='fr_da_rech30', y='fr_da_rech90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='fr_ma_rech30', y='fr_ma_rech90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='medianamnt_ma_rech30', y='medianamnt_ma_rech90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='daily_decr30', y='daily_decr90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='rental30', y='rental90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='payback30', y='payback90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='medianmarechprebal30', y='medianmarechprebal90', data=df, hue='label')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.scatterplot(x='sumamnt_ma_rech30', y='sumamnt_ma_rech90', data=df, hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the scatter plot we are able to notice the data distribution and success failure points on those feature columns also showing any kind of outlier details present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-bright')\n",
    "\n",
    "df.hist(figsize=(20,30))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observation:\n",
    "\n",
    "***`for feature aon:`***\n",
    "- Data ranges from -48 to 999860 with Mean value of 8112.34.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature daily_descr30:`***\n",
    "- Data ranges from -93 to 265926 with Mean value of 5381.4.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature daily_descr90:`***\n",
    "- Data ranges from -93 to 320630 with Mean value of 6082.52.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature rental30:`***\n",
    "- Data ranges from -23737.14 to 198926 with Mean value of 2692.58.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature rental90:`***\n",
    "- Data ranges from -24720 to 200148 with Mean value of 3483.41.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature last_rech_date_ma:`***\n",
    "- Data ranges from -29 to 998650 with Mean value of 3755.85.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature last_rech_date_da:`***\n",
    "- Data ranges from -29 to 999178 with Mean value of 3712.2.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature last_rech_amt_ma:`***\n",
    "- Data ranges from 0 to 55000 with Mean value of 2064.45.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_ma_rech30:`***\n",
    "- Data ranges from 0 to 203 with Mean value of 3.98.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature fr_ma_rech30:`***\n",
    "- Data ranges from 0 to 999606 with Mean value of 3737.36.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature sumamnt_ma_rech30:`***\n",
    "- Data ranges from 0 to 810096 with Mean value of 7704.5.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianamnt_ma_rech30:`***\n",
    "- Data ranges from 0 to 55000 with Mean value of 1812.82.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianmarechprebal30:`***\n",
    "- Data ranges from -200 to 999479 with Mean value of 3851.93.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_ma_rech90:`***\n",
    "- Data ranges from 0 to 336 with Mean value of 6.32.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature fr_ma_rech90:`***\n",
    "- Data ranges from 0 to 88 with Mean value of 7.72.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature sumamnt_ma_rech90:`***\n",
    "- Data ranges from 0 to 953036 with Mean value of 12396.22.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianamnt_ma_rech90:`***\n",
    "- Data ranges from 0 to 55000 with Mean value of 1864.6.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianmarechprebal90:`***\n",
    "- Data ranges from -200 to 41456 with Mean value of 92.03.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_da_rech30:`***\n",
    "- Data ranges from 0 to 99914 with Mean value of 262.58.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature fr_da_rech30:`***\n",
    "- Data ranges from 0 to 999809 with Mean value of 3749.49.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is highly spreaded and needs to be treated accordingly.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_da_rech90:`***\n",
    "- Data ranges from 0 to 38 with Mean value of 0.04.\n",
    "- Data is distributed normally but not in well curve.\n",
    "- Data is positively skewed and need to be treated accordingly.\n",
    "\n",
    "***`for feature fr_da_rech90:`***\n",
    "- Data ranges from 0 to 64 with Mean value of 0.05.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_loans30:`***\n",
    "- Data ranges from 0 to 50 with Mean value of 2.76.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature amnt_loans30:`***\n",
    "- Data ranges from 0 to 306 with Mean value of 17.95.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature maxamnt_loans30:`***\n",
    "- Data ranges from 0 to 99864 with Mean value of 274.66.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianamnt_loans30:`***\n",
    "- Data ranges from 0 to 3 with Mean value of 0.05.\n",
    "- Data is not distributed normally or in well curve and it is understandable as feature has only limited set of values.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature cnt_loans90:`***\n",
    "- Data ranges from 0 to 4997.52 with Mean value of 18.52.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature amnt_loans90:`***\n",
    "- Data ranges from 0 to 438 with Mean value of 23.65.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature maxamnt_loans90:`***\n",
    "- Data ranges from 0 to 12 with Mean value of 6.7.\n",
    "- Data is not distributed normally or in well curve and it understandable as user has two option for loans i.e., 5 and 10 for with 6 and 12 has to be paid.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature medianamnt_loans90:`***\n",
    "- Data ranges from 0 to 3 with Mean value of 0.05.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature payback30:`***\n",
    "- Data ranges from 0 to 171.5 with Mean value of 3.4.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly.\n",
    "\n",
    "***`for feature payback90:`***\n",
    "- Data ranges from 0 to 171.5 with Mean value of 4.32.\n",
    "- Data is not distributed normally or in well curve.\n",
    "- Data is positively skewed and needs to be treated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation using a Heatmap\n",
    "Positive correlation - A correlation of +1 indicates a perfect positive correlation, meaning that both variables move in the same direction together.\n",
    "Negative correlation - A correlation of –1 indicates a perfect negative correlation, meaning that as one variable goes up, the other goes down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_triangle = np.triu(df.corr())\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True, square=True, fmt='0.3f', \n",
    "            annot_kws={'size':10}, cmap=\"cubehelix\", mask=upper_triangle)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above heatmap due to lot of columns we are not able to see the correlation details however we can observe the color coding details and get a hint that there is no multi collinearity concern between the column values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Bar Plot comparing Gender column with the remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = df.corr()\n",
    "plt.figure(figsize=(15,5))\n",
    "df_corr['label'].sort_values(ascending=False).drop('label').plot.bar()\n",
    "plt.title(\"Correlation of Feature columns vs Label\\n\", fontsize=16)\n",
    "plt.xlabel(\"\\nFeatures List\", fontsize=14)\n",
    "plt.ylabel(\"Correlation Value\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pdate'] = pd.to_datetime(df['pdate']) # converting from object to datetime\n",
    "\n",
    "df['Year']=df['pdate'].dt.year\n",
    "df['Month']=df['pdate'].dt.month\n",
    "df['Date']=df['pdate'].dt.day\n",
    "\n",
    "print(df['Year'].value_counts())\n",
    "print(df['Month'].value_counts())\n",
    "print(df['Date'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['msisdn', 'pdate', 'Year'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,30))\n",
    "outl_df = df.columns.values\n",
    "for i in range(0, len(outl_df)):\n",
    "    plt.subplot(7, 5, i+1)\n",
    "    ax = sns.boxenplot(df[outl_df[i]], color='red')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the help of boxen plot we can see the outlier details present in our numerical data columns. However when I tried removing the outliers using Z score or IQR methods I was losing close to 25 percent data therefore retaining all the data points as it is to avoid any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the skew method we see that there are columns present in our dataset that are above the acceptable range of +/-0.5 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,30))\n",
    "for i in range(0, len(outl_df)):\n",
    "    plt.subplot(7, 5, i+1)\n",
    "    ax = sns.distplot(df[outl_df[i]], color='blue')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of distribution plot we can see the skewness details present in our numerical data columns. I tried using log transformation to reduce the skewness however that created NaN values for our dataset and hence I am trying to avoid that situation by using this slightly skewed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into 2 variables namely 'X' and 'Y' for feature and label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have bifurcated the dataset into features and labels where X represents all the feature columns and Y represents the target label column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving the class imbalance issue in our label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_label = Y.value_counts()\n",
    "old_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the values of our label column to count the number of rows occupied by each category. This indicates class imbalance that we will need to fix by using the oversampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding samples to make all the categorical label values same\n",
    "\n",
    "oversample = SMOTE()\n",
    "X, Y = oversample.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SMOTE` is the over sampling mechanism that we are using to ensure that all the categories present in our target label have the same number of rows covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_label = Y.value_counts()\n",
    "new_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying over sampling we are once again listing the values of our label column to cross verify the updated information. Here we see that we have successfully resolved the class imbalance problem and now all the categories have same data ensuring that the machine learning model does not get biased towards one category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am scaling my feature data to ensure that there is no issue with the data biasness over a particular column instead a standardization will occur helping us in having a uniform dataset value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding best random state for building Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAccu=0\n",
    "maxRS=0\n",
    "\n",
    "for i in range(1, 200):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=i)\n",
    "    lr=LogisticRegression()\n",
    "    lr.fit(X_train, Y_train)\n",
    "    pred = lr.predict(X_test)\n",
    "    acc_score = (accuracy_score(Y_test, pred))*100\n",
    "    \n",
    "    if acc_score>maxAccu:\n",
    "        maxAccu=acc_score\n",
    "        maxRS=i\n",
    "\n",
    "print(\"Best accuracy score is\", maxAccu,\"on Random State\", maxRS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance bar graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(X_train, Y_train)\n",
    "importances = pd.DataFrame({'Features':X.columns, 'Importance':np.round(rf.feature_importances_,3)})\n",
    "importances = importances.sort_values('Importance', ascending=False).set_index('Features')\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "importances.plot.bar(color='teal')\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "importances.plot.bar(color='teal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop([\"last_rech_date_da\", \"cnt_da_rech90\", \"cnt_da_rech30\", \"fr_da_rech30\", \"fr_da_rech90\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the training and testing data sets with optimum Random State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Model for Classification with Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Model Function\n",
    "\n",
    "def classify(model_func):\n",
    "    \n",
    "    for model_name, model in model_func.items():\n",
    "        \n",
    "        # Training the model\n",
    "        model.fit(X_train, Y_train)\n",
    "    \n",
    "        # Predicting Y_test\n",
    "        pred = model.predict(X_test)\n",
    "        \n",
    "        print('\\n##############################',model_name,'##############################')\n",
    "        \n",
    "        # Classification Report\n",
    "        class_report = classification_report(Y_test, pred)\n",
    "        print(\"\\nClassification Report for {}:\\n\".format(model_name), class_report)\n",
    "    \n",
    "        # Accuracy Score\n",
    "        acc_score = (accuracy_score(Y_test, pred))*100\n",
    "        print(\"Accuracy Score for {}:\".format(model_name), acc_score)\n",
    "    \n",
    "        # Cross Validation Score\n",
    "        cv_score = (cross_val_score(model, X, Y, cv=5).mean())*100\n",
    "        print(\"Cross Validation Score for {}:\".format(model_name), cv_score)\n",
    "    \n",
    "        # Result of accuracy minus cv scores\n",
    "        result = acc_score - cv_score\n",
    "        print(\"\\nAccuracy Score - Cross Validation Score is\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have defined a class that will perform the train-test split, training of machine learning model, predicting the label value, getting the accuracy score, generating the classification report, getting the cross validation score and the result of difference between the accuracy score and cross validation score for any machine learning model that calls for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "ETC = ExtraTreesClassifier()\n",
    "SVCM = SVC(C=1.0, kernel='rbf', gamma='auto', random_state=42)\n",
    "DTC = DecisionTreeClassifier(max_depth=15, random_state=21)\n",
    "RFC = RandomForestClassifier(max_depth=15, random_state=111)\n",
    "KNN = KNeighborsClassifier(n_neighbors=15)\n",
    "XGB = xgb.XGBClassifier(verbosity=0)\n",
    "LGBM = lgb.LGBMClassifier()\n",
    "\n",
    "models = {'Logistic Regression' : LR,\n",
    "         'Extra Trees Classifier' : ETC,\n",
    "         'Support Vector Classifier' : SVCM,\n",
    "         'Decision Tree Classifier' : DTC,\n",
    "         'Random Forest Classfier' : RFC,\n",
    "         'K Nearest Neighbors Classifier' : KNN,\n",
    "         'XGB Classifier' : XGB,\n",
    "         'LGBM Classifier' : LGBM}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disp = plot_roc_curve(LR, X_test, Y_test)\n",
    "# plot_roc_curve(ETC, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(SVCM, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(DTC, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(RFC, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(KNN, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(XGB, X_test, Y_test, ax=disp.ax_)\n",
    "# plot_roc_curve(LGBM, X_test, Y_test, ax=disp.ax_)\n",
    "# plt.legend(prop={'size':10}, loc = 'best')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning on the best Classification ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing Extra Trees Classifier\n",
    "\n",
    "fmod_param = {'criterion' : [\"gini\", \"entropy\"],\n",
    "              'max_depth' : [30, 40],\n",
    "              'n_estimators' : [300, 350],\n",
    "              'min_samples_split' : [3, 4],\n",
    "              'random_state' : [42, 72]\n",
    "             }\n",
    "\n",
    "GSCV = GridSearchCV(ExtraTreesClassifier(), fmod_param, cv=5)\n",
    "GSCV.fit(X_train,Y_train)\n",
    "GSCV.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After comparing all the classification models I have selected Extra Trees Classifier as my best model and have listed down it's parameters above referring the sklearn webpage. I am using the Grid Search CV method for hyper parameter tuning my best model. I have trained the Grid Search CV with the list of parameters I feel it should check for best possible outcomes. So the Grid Search CV has provided me with the best parameters list out of all the combinations it used to train the model that I can use on my final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Model = ExtraTreesClassifier(criterion=\"entropy\", max_depth=30, min_samples_split=3, \n",
    "                                   n_estimators=350, random_state=72)\n",
    "Classifier = Final_Model.fit(X_train, Y_train)\n",
    "fmod_pred = Final_Model.predict(X_test)\n",
    "fmod_acc = (accuracy_score(Y_test, fmod_pred))*100\n",
    "print(\"Accuracy score for the Best Model is:\", fmod_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC ROC Curve¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp = metrics.plot_roc_curve(Final_Model, X_test, Y_test)\n",
    "disp.figure_.suptitle(\"ROC Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confusion matrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_confusion_matrix(Classifier, X_test, Y_test, cmap='mako')\n",
    "plt.title('\\t Confusion Matrix for the Final Model \\n')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"FinalModel_MicroCreditLoanDefaulter.pkl\"\n",
    "joblib.dump(Final_Model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = joblib.load(filename)\n",
    "result = load_model.score(X_test, Y_test)*100\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am loading the previously saved final model to test the score on our testing dataset. But it can be used to test the model on unseen data values as well and then predict the label accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "> Key Findings and Conclusions of the Study: \n",
    "From the final model MFI can find if a person will return money or not and should a MFI provide a load to that person or not judging from the various features taken into consideration\n",
    "\n",
    "> Learning Outcomes of the Study in respect of Data Science: \n",
    "I built multiple classification models and did not rely on one single model for getting better accuracy and using cross validation comparison I ensured that the model does not fall into overfitting and underfitting issues. I picked the best one and performed hyper parameter tuning on it to enhace the scores.\n",
    "\n",
    "> Limitations of this work and Scope for Future Work: \n",
    "Limitation is it will only work for this particular use case and will need to be modified if tried to be utilized on a different scenario but on a similar scale.\n",
    "Scope is that we can use it in companies to find whether we should provide loan to a person or not and we can also make prediction about a person buying an expensive service on the basis of there personal details that we have in this dataset like number of times data account got recharged in last 30 days and daily amount spent from main account, averaged over last 30 days (in Indonesian Rupiah) so even a marketing company can also use this.\n",
    "\n",
    "![Thank-you.jpg](attachment:Thank-you.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
